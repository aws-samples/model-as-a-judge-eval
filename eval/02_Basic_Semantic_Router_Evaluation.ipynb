{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6cf8ad0-28e7-4195-9d85-06ddad4c64b5",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The purpose of this notebook is to create a test suite for a semantic router / classifer. It allows us to test our router prompt against the routes we anticapate to ensure it's behaving correctly.\n",
    "\n",
    "This notebook calls Bedrock using our router prompt against a set of 100 example user queries to ensure the correct route is selected.\n",
    "\n",
    "## Pre-Requisites\n",
    "\n",
    "Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "access Amazon Bedrock\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "1. AmazonBedrockFullAccess\n",
    "\n",
    "## Note\n",
    "Running this notebook will incur charges from calling Bedrock. There are 100 example chat conversations which means there will be 100 calls to a Bedrock LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c740bdf-e447-480b-886c-142540230215",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f96929-2fc7-42da-8e83-8125c98012b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required dependencies if you haven't already done so from the requirements.txt\n",
    "\n",
    "# %pip install -U scikit-learn==1.4.2\n",
    "# %pip install -U langchain==0.1.13\n",
    "# %pip install -U pandas==2.2.2\n",
    "# %pip install -U matplotlib=3.8.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4742cc-b8f0-4b0d-90d1-531431093d57",
   "metadata": {},
   "source": [
    "# Create Eval Dataset\n",
    "\n",
    "This part is a little tricky and time consuming. **For the purpose of this notebook, we went ahead and created a dataset**. We did this by prompting an LLM to generate synthetic user questions so we could test our router.\n",
    "\n",
    "We recommend currating your own examples by interacting with your own chat bot to ensure a robust dataset. We also recommend you add to this eval dataset over time.\n",
    "\n",
    "\n",
    "Next lets load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257625b-d458-4563-92b4-0aba6dce983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "router_prompts = '../data/router_inputs.csv'\n",
    "\n",
    "df = pd.read_csv(router_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060351c-648c-4cbf-af78-074c0d96b436",
   "metadata": {},
   "source": [
    "## Define our router's paths. \n",
    "Our router paths all contain a name and description (like tools) and should contain an invoke() method. We will define 3 possible paths to test our router against. \n",
    "\n",
    "The action agent is an agent that can make API calls to interact with a system. The RAGAgent queries a knowledge base. And the fallback function handles all other requests such as jailbreaking attempts or behavior that the system does not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8fbb4-a6b0-4153-b499-d39996d41d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod \n",
    "\n",
    "class BaseFunction(ABC):\n",
    "\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "    @abstractmethod\n",
    "    def invoke(self, input: str) -> str:\n",
    "        pass\n",
    "\n",
    "class ActionAgent(BaseFunction):\n",
    "\n",
    "    name: str = 'ActionAgent'\n",
    "    description: str =  'Useful when a user is asking the system to perform an action in the outside world such as \"submit a time off request for me\"'\n",
    "\n",
    "    def invoke(self, input: str) -> str:\n",
    "        # Placeholder. We don't actually need the invoke function for this experiment.\n",
    "        return ''\n",
    "\n",
    "class RAGAgent(BaseFunction):\n",
    "\n",
    "    name: str = 'RAGAgent'\n",
    "    description: str = 'Useful when a user is asking a question that can be found in a knowledge base.'\n",
    "\n",
    "    def invoke(self, input: str) -> str:\n",
    "        # Placeholder. We don't actually need the invoke function for this experiment.\n",
    "        return ''\n",
    "\n",
    "class ClarificationFunction(BaseFunction):\n",
    "\n",
    "    name: str = 'ClarificationFunction'\n",
    "    description: str = \"Useful for when the user asks a question, but it's unclear what their specific ask is. This tool will then ask for clarification before selecting a more appropriate tool.\"\n",
    "\n",
    "    def invoke(self, input: str) -> str:\n",
    "        # Placeholder. We don't actually need the invoke function for this experiment.\n",
    "        return ''\n",
    "\n",
    "class FallbackFunction(BaseFunction):\n",
    "\n",
    "    name: str = 'FallbackFunction'\n",
    "    description: str = 'Useful as a fall back for when other tool descriptions don\\'t seem correct. This is a last resort option. If a user asks something harmful, select this tool as well.'\n",
    "\n",
    "    def invoke(self, input: str) -> str:\n",
    "        # Placeholder. We don't actually need the invoke function for this experiment.\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc306b8-05f2-4085-8036-40aafed1e750",
   "metadata": {},
   "source": [
    "# Define our Router Prompt\n",
    "This is a router prompt that asks the model to select the most appropriate tool. There are plenty of packages like Langchain that support this type of routing. Often times, they're not flexible enough for our needs so we'll write it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255732a-83ac-4220-97e5-d19cb62f37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "ROUTER_SYS_PROMPT = '''You are a a helpful assistant that is given access to tool definitions. Your task is to take in tool name and definitions, and select \n",
    "the most appropriate tool to use to answer a users question. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "The decision you make should be based on one of these tool names: \n",
    "{tool_names}\n",
    "\n",
    "Select the best tool given the human input below and respond in json using the format below: \n",
    "{{\"toolName\": <tool name>}}\n",
    "\n",
    "DO NOT return anything other than json.\n",
    "\n",
    "If the user requests something that isn't related to an automation or internal policy document, use the fallback tool'''\n",
    "\n",
    "USER_SYS_PROMPT = '''Using the users request below\n",
    "\n",
    "<user_request>\n",
    "{input}\n",
    "</user_request>\n",
    "\n",
    "Select the most appropriate tool and respond in the json format described above.'''\n",
    "\n",
    "\n",
    "router_prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    ('system', ROUTER_SYS_PROMPT),\n",
    "    ('human', USER_SYS_PROMPT)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3fcc8-7c19-4012-b1a9-5944ba5d97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import json\n",
    "\n",
    "HYPER_PARAMS = {\n",
    "    \"temperature\": 0.3, \n",
    "    \"top_k\": 100,\n",
    "}\n",
    "\n",
    "class SemanticRouter:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Lets define all the routes that this router has access to.\n",
    "        self.routes: list[BaseFunction]  = [\n",
    "            ActionAgent(),\n",
    "            RAGAgent(),\n",
    "            ClarificationFunction(),\n",
    "            FallbackFunction()\n",
    "        ]\n",
    "\n",
    "        # Grab the prompt we just created.\n",
    "        self.router_prompt: ChatPromptTemplate = router_prompt\n",
    "\n",
    "        self.client: BedrockChat = BedrockChat(\n",
    "            model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "            model_kwargs=HYPER_PARAMS\n",
    "        )\n",
    "\n",
    "\n",
    "    def _get_route(self, input: str) -> BaseFunction:\n",
    "        \n",
    "        # Gather the input variables for the prompt\n",
    "        tools: str = '\\n'.join([f'{t.name}: {t.description}' for t in self.routes])\n",
    "        tool_names: str = ', '.join([t.name for t in self.routes])\n",
    "\n",
    "        # Lets use our router prompt. Langchain will return a dict with text containing our response.\n",
    "        messages: BaseMessage = self.router_prompt.format_messages(\n",
    "            input=input,\n",
    "            tools=tools,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "        response: AIMessage = self.client.invoke(messages)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            # Parse the response. If it's malformed, it'll show up in the except clause.\n",
    "            response_json: dict = json.loads(response.content)            \n",
    "            # We expect the model to return json containing the tool name (see prompt)\n",
    "            tool_name: str = response_json['toolName']\n",
    "            # Iterate through the the pipelines to find the match.           \n",
    "            route: BasePipeline = next((r for r in self.routes if r.name == tool_name), None)\n",
    "            # If no match is found, it's because the json response was incorrect.\n",
    "            if not route:\n",
    "                return 'Requested route was malformed'\n",
    "            \n",
    "            return route.name\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f'Could not find an appropriate route for the users request.\\n\\nError: {e}'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44a4ed-2f14-446a-b9ae-f94fa2aaf306",
   "metadata": {},
   "source": [
    "## Test Out The Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135ea5c-1c72-4d06-bff0-aeb889fc7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "router = SemanticRouter()\n",
    "\n",
    "router._get_route('Can you submit time off for me?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb38c61-3bf8-45a5-86a2-cca61b1da35d",
   "metadata": {},
   "source": [
    "# Helper Functions For Bedrock\n",
    "In the section below we'll define some helper functions to speed up the evaluation process. We'll call bedrock from a threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1edd8-52fa-4e06-8130-059e114860b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bit funky. We're dumping all the requests into a thread pool\n",
    "# And storing the index for the order in which they were submitted. \n",
    "# Lastly, we're inserting them into the response array at their index to ensure order.\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "# We only care about the response from the semantic router so we'll call the _get_route() example\n",
    "def bedrock_call(input):\n",
    "    router = SemanticRouter()\n",
    "    return router._get_route(input)\n",
    "    \n",
    "\n",
    "def call_bedrock_threaded(requests, max_workers=5):\n",
    "    # Dictionary to map futures to their position\n",
    "    future_to_position = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all requests and remember their order\n",
    "        for i, request in enumerate(requests):\n",
    "            future = executor.submit(bedrock_call, request)\n",
    "            future_to_position[future] = i\n",
    "        \n",
    "        # Initialize an empty list to hold the responses\n",
    "        responses = [None] * len(requests)\n",
    "        \n",
    "        # As each future completes, assign its result to the correct position\n",
    "        for future in as_completed(future_to_position):\n",
    "            position = future_to_position[future]\n",
    "            try:\n",
    "                response = future.result()\n",
    "                responses[position] = response\n",
    "            except Exception as exc:\n",
    "                print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                responses[position] = None  # Or handle the exception as appropriate\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64500d82-74ce-4032-8abc-3791618d4693",
   "metadata": {},
   "source": [
    "# Run Evaluations\n",
    "\n",
    "Lets get validation results. For this notebook, we're looking to see how often the model routes correctly. Expect the calls to take ~15 seconds since the model is just outputting the path the take for each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84601265-0242-42b0-91bd-3f87836300dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries. This is easier to work with in our threaded code.\n",
    "input_records: list[dict] = df.to_dict('records')\n",
    "\n",
    "# Create prompts for all of our records.\n",
    "requests: list[str] = [r['User Request'] for r in input_records]\n",
    "\n",
    "# Call Bedrock threaded to speed up getting all our responses.\n",
    "responses: list[AIMessage] = call_bedrock_threaded(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054013b-3db3-4709-9bdf-dbdfa440a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the responses back into the records and recreate the valuation\n",
    "for i,r in enumerate(input_records):\n",
    "    r['Model Response'] = responses[i]\n",
    "\n",
    "\n",
    "evaluation_df = pd.DataFrame(input_records)\n",
    "evaluation_df = evaluation_df.rename(columns={'Action': 'Ground Truth'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39a3c6-2117-49cb-9d90-755a6d2b2069",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "Show the cross tab for what it's getting wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589beb09-d245-45d8-bf30-acb01edb8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'Action' is your actual label and 'Model Response' is the predicted label\n",
    "actual = evaluation_df['Ground Truth']\n",
    "predicted = evaluation_df['Model Response']\n",
    "\n",
    "pd.crosstab(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb4e99-b326-4a2a-a129-4fbb1599abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total accuracy\n",
    "correct = (actual == predicted).sum()\n",
    "total = len(evaluation_df)\n",
    "accuracy = correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb453e-944a-42bc-b9b6-106e0765b0e5",
   "metadata": {},
   "source": [
    "# Human Eval\n",
    "Based on the accuracy, you should have somewhere around ~76%. In the section below, we'll subsample ~10 incorrect responses to help understand where the router is failing and what descriptions to change to make it work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c955e3-f26e-46e1-a31c-364950847bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drill down into the incorrect answers to see what happened.\n",
    "\n",
    "# Identify mismatches\n",
    "mismatches = evaluation_df[evaluation_df['Ground Truth'] != evaluation_df['Model Response']]\n",
    "\n",
    "# Convert to HTML\n",
    "html_table = mismatches.to_html(index=False)\n",
    "\n",
    "# Optional: Add CSS styling\n",
    "html_table = f\"\"\"\n",
    "<style>\n",
    "    table, th, td {{\n",
    "        border: 1px solid black;\n",
    "        border-collapse: collapse;\n",
    "        padding: 8px;\n",
    "        text-align: left;\n",
    "    }}\n",
    "    th {{\n",
    "        background-color: #f2f2f2;\n",
    "    }}\n",
    "</style>\n",
    "{html_table}\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML table in a Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(html_table))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
