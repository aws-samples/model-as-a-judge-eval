{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b47ece3",
   "metadata": {},
   "source": [
    "# Overview \n",
    "In this notebook, we will show use the model as a judge pattern for evaluating chat / agent responses. We will be evaluating an agents ability to help users with finding and purchasing tickets to events.\n",
    "\n",
    "In this notebook we will:\n",
    "1. Define qualitative metrics that make sense for our usecase\n",
    "1. Create a quality assurance rubric that an large language model (LLM) can use to grade qualitatively\n",
    "1. Run our test Suite\n",
    "1. Validate the results and understand what needs to change in the system to improve the overall user experience.\n",
    "\n",
    "\n",
    "## Building the Evaluation Framework\n",
    "The process for evaluating a chat \"agent\" is slightly different than single turn Q&A. On top of vending factually accurate results, we also care about the customer experience, tone, and whether the agent confirms with the user before purchasing tickets. In the example dataset, there are 50 made up chat conversations a human had with an chat model. To  use this for a different usecase, you'd want to gather 50+ examples from your agent and swap out the fake chat conversations with your own.\n",
    "\n",
    "Resist the temptation to grab a premade list! Using your own questions from your use case will make a huge difference.\n",
    "\n",
    "When evaluating a prompt, we want **at least** 50+ examples to run benchmarks on. One of the best ways to differentiate between a gen AI science project and a viable product is to count the number of automated tests. A handful of manual tests? Science Project. An automated system of hundreds of tests that runs every time you propose a change? Production ready.\n",
    "\n",
    "**Note** In subsequent notebooks, we will show you how to compare conversations between two different chat models. For this notebook, we are just evaluating a single agent.\n",
    "\n",
    "\n",
    "## Pre-Requisites\n",
    "\n",
    "Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "\n",
    "access Amazon Bedrock\n",
    "access to Amazon OpenSearch Serverless\n",
    "\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "\n",
    "1. AmazonBedrockFullAccess\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f81e6b",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required dependencies if you haven't already done so from the requirements.txt\n",
    "\n",
    "# %pip install -U boto3==1.34.82\n",
    "# %pip install -U langchain==0.1.13\n",
    "# %pip install -U pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0937924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb513ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14d0f0",
   "metadata": {},
   "source": [
    "# Create Eval Dataset\n",
    "\n",
    "This part is a little tricky and time consuming. **For the purpose of this notebook, we went ahead and created a dataset**. We did this by prompting an LLM to generate fictitious chat conversations both good and bad.\n",
    "\n",
    "We recommend currating additional examples by interacting with your chat bot to ensure a robust dataset. We also recommend you add to this eval dataset over time.\n",
    "\n",
    "\n",
    "Next lets import our dataset into pandas and take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76467684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "lines = []\n",
    "with open('../data/chat_evaluation_data.jsonl') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "chat_conversations = [json.loads(line) for line in lines]\n",
    "\n",
    "print(chat_conversations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69693f60",
   "metadata": {},
   "source": [
    "# Define Metrics for Benchmarking\n",
    "\n",
    "In this step, we'll be defining the metrics we care about in order to benchmark both different prompts and models. \n",
    "\n",
    "There are two main types of evaluation metrics. Qualitative and quantitative. Find descriptions of each type below. \n",
    "\n",
    "### Quantitative \n",
    "Quantitative metrics involve numerical measurements that can objectively compare different models. These typically include accuracy, perplexity, speed, and resource efficiency, among others. They provide a clear, standardized way to measure certain aspects of an LLM's performance, such as how well it predicts the next word in a sequence or how quickly it generates responses. \n",
    "\n",
    "### Qualitative\n",
    "On the other hand, qualitative metrics assess the more subjective aspects of LLM performance, including the coherence, relevancy, creativity of generated text, and adherence to ethical guidelines. These are often evaluated through human judgment via methods such as expert reviews or user studies, offering insights into the user experience and the contextual appropriateness of the model's outputs. While quantitative metrics can offer precise, measurable benchmarks, qualitative metrics are crucial for understanding the nuances and real-world effectiveness of LLMs. \n",
    "\n",
    "For qualitative evals we will want to consider\n",
    "1. Coherence of response\n",
    "2. Relevance of information returned\n",
    "3. Accuracy of response\n",
    "4. Adherance to brand guidelines\n",
    "\n",
    "\n",
    "## How do we gather qualitative metrics?\n",
    "\n",
    "To gather qualitative metrics, we have two options. (1) Create a QA rubrik and give it to human evaluators or (2) Use that same rubrik and give it to an LLM to evaluate the responses. \n",
    "\n",
    "As a test suite gets larger, human evaluation becomes a bottleneck. Grading 500+ answers every time you make a change to a prompt is not scalable. Because of this, we'll opt to use an LLM to evaluate our responses. For poorly scoring responses, we can then manually check to see what's going on and fix the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe3a6c",
   "metadata": {},
   "source": [
    "## Lets create a grading prompt\n",
    "Below you'll find a prompt that takes in the question, model response, correct answer, and a rubric that you will create to evaluate models output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26305b2b-b0bc-4de2-bee6-c8c675124575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your rubric here.\n",
    "RUBRIC = '''\n",
    "- The conversation should have a friendly tone and not be overly verbose. \n",
    "- If the user asks to purchase a ticket, the assistant should get confirmation before executing a transaction.\n",
    "- If a customer asks about something not related to an event, the model should respond indicating to the user that it cannot help them.\n",
    "- The conversation should be less than 5 turns at most to complete the conversation. The  assistant should not get stuck asking the user to repeat things already discussed in the conversation.\n",
    "- Any recommendation for events or artists should be relevant to the conversation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(conversation: str) -> BaseMessage:\n",
    "    prompt = \"\"\"You will be provided a chat conversation that an assistant had with a user, and a rubric that instructs you on what makes the conversation correct or incorrect.\n",
    "    \n",
    "    Here is the conversation.\n",
    "    <conversation>\n",
    "    {conversation}\n",
    "    </conversation>\n",
    "    \n",
    "    Here is the rubric on how to grade the assistant response.\n",
    "    <rubric>\n",
    "    {rubric}\n",
    "    </rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\n",
    "    \"\"\"\n",
    "\n",
    "    # First we will generate a prompt template using Langchain and the prompt above\n",
    "    chat_template: ChatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", prompt)\n",
    "    ])\n",
    "        \n",
    "    # Next, we will insert all the variables into into the prompt. \n",
    "    return chat_template.format_messages(\n",
    "        conversation=conversation,\n",
    "        rubric=RUBRIC\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6a76a-0259-4c75-a1dc-ceacbe022cf3",
   "metadata": {},
   "source": [
    "# Helper Functions For Bedrock\n",
    "In the section below we'll define some helper functions to speed up the evaluation process. We'll call bedrock from a threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SONNET_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "HAIKU_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# To flip between different models, you can change these global variable.\n",
    "MODEL_TO_USE = SONNET_ID\n",
    "\n",
    "REGION = 'us-west-2'\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "def call_bedrock(request: BaseMessage):\n",
    "    client = BedrockChat(\n",
    "        model_id=MODEL_TO_USE, \n",
    "        model_kwargs= {\"temperature\": 0.5, \"top_k\": 500}\n",
    "    )\n",
    "    \n",
    "    response = client.invoke(request)\n",
    "    return response\n",
    "\n",
    "# This is a bit funky. We're dumping all the requests into a thread pool\n",
    "# And storing the index for the order in which they were submitted. \n",
    "# Lastly, we're inserting them into the response array at their index to ensure order.\n",
    "def call_threaded(requests, function):\n",
    "    # Dictionary to map futures to their position\n",
    "    future_to_position = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit all requests and remember their order\n",
    "        for i, request in enumerate(requests):\n",
    "            future = executor.submit(function, request)\n",
    "            future_to_position[future] = i\n",
    "        \n",
    "        # Initialize an empty list to hold the responses\n",
    "        responses = [None] * len(requests)\n",
    "        \n",
    "        # As each future completes, assign its result to the correct position\n",
    "        for future in as_completed(future_to_position):\n",
    "            position = future_to_position[future]\n",
    "            try:\n",
    "                response: AIMessage = future.result()\n",
    "                responses[position] = response.content\n",
    "            except Exception as exc:\n",
    "                print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                responses[position] = None  # Or handle the exception as appropriate\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05e837",
   "metadata": {},
   "source": [
    "# Run Evaluations\n",
    "\n",
    "Lets get validation results. For this notebook, we're just looking to calculate correctness. Expect the calls to take ~30 seconds since the model is output it's reasoning as well as whether it's correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper conversation\n",
    "def conversation_to_str(conversation: list[dict]) -> str:\n",
    "    return ''.join([f\"{c['type']}: {c['text']}\" for c in conversation])\n",
    "\n",
    "# Construct grader prompts from the chat conversations\n",
    "grader_prompts = []\n",
    "for i, c in enumerate(chat_conversations):    \n",
    "    conversation_str: str = conversation_to_str(c)\n",
    "    prompt: BaseMessage = build_grader_prompt(conversation_str)\n",
    "    grader_prompts.append(prompt)\n",
    "\n",
    "\n",
    "# Call Bedrock threaded to speed up getting all our responses. The results should come back in order.\n",
    "evaluation_results: list[str] = call_threaded(grader_prompts, call_bedrock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d768a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Strip out the correctness grade\n",
    "def extract_correctness(response):\n",
    "    # Regular expression to extract everything inside of the sumologquery tags\n",
    "    regex = r'<correctness>(.*?)</correctness>'\n",
    "    # Perform the regex search\n",
    "    matches = re.search(regex, response, re.DOTALL)\n",
    "    # Extract the matched content, if any\n",
    "    return matches.group(1).strip() if matches else None\n",
    "\n",
    "# Strip out the reasoning\n",
    "def extract_reasoning(response):\n",
    "    # Regular expression to extract everything inside of the sumologquery tags\n",
    "    regex = r'<thinking>(.*?)</thinking>'\n",
    "    # Perform the regex search\n",
    "    matches = re.search(regex, response, re.DOTALL)\n",
    "    # Extract the matched content, if any\n",
    "    return matches.group(1).strip() if matches else None\n",
    "    \n",
    "\n",
    "def format_results(grade: str, chat_conversation: list[dict]) -> dict:\n",
    "    reasoning: str = extract_reasoning(grade)\n",
    "    correctness: str =  extract_correctness(grade)\n",
    "    \n",
    "    return {\n",
    "        'chat_conversation': chat_conversation,\n",
    "        'reasoning': reasoning,\n",
    "        'correctness': correctness\n",
    "    }\n",
    "\n",
    "\n",
    "formatted_results = [format_results(g, chat_conversations[i]) for i, g in enumerate(evaluation_results)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "evaluated_df = pd.DataFrame(formatted_results)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ab469",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Now that we have our new evaluation dataframe, lets do an analysis on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827c258",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Next lets see how many we got correct\n",
    "percentage_correct = evaluated_df['correctness'].value_counts(normalize=True)['correct'] * 100\n",
    "print(f\"Percentage correct: {percentage_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcadd65",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edbe6df-6966-493a-aa9f-fcd86afe7622",
   "metadata": {},
   "source": [
    "# Human Eval\n",
    "Based on the correctness score, you should have somewhere around ~60%. In the sample dataset, we explicitly made some conversations which would not pass the rubric to show you how human evaluation comes into play. In the section below, we'll subsample ~10 incorrect responses to help understand where the agent is failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b77849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we need to do some human evaluation. Lets sample a subsection of 10 incorrect responses\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assuming you have a dataframe called 'df' with a column called 'result'\n",
    "incorrect_rows = evaluated_df[evaluated_df['correctness'] == 'incorrect'].sample(n=10)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = incorrect_rows.to_html(index=False, classes='table table-striped')\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6abb38",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that you've run through this notebook. \n",
    "* Go back and play with the rubric. \n",
    "* You can also play with the temperature and other hyperparameters of the model to see how that has an effect on your score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b43f5-a503-44b7-aea1-f6869892d40c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook we did a basic qualitative evaluation with an LLM. The dataset provided intentionally has some conversations that would not pass the evaulation to show how a combination of \"model as a judge\" and human evaluation are needed to understand performance, diagnose issues, and understand what needs to change in the system to improve the overall user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043989f-6861-4051-b4fe-2cf6ffc0477f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
